{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "name": "Capstone Project using LSTM.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivgPOTeVEHJ",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Comment Detection with PyTorch LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBUQYdEgVEHL",
        "colab_type": "text"
      },
      "source": [
        "In this kernel, I implement LSTM for toxicity prediction using only PyTorch (and torchtext) whenever possible. I have been a keras user since forever, but I notice more and more new architectures are built through PyTorch. Thus, the main purpose of building this kernel for me is to learn PyTorch. I leave some comments to explain what each section is trying to do, hopefully it can help you too.\n",
        "\n",
        "I incorporate many techniques that I have learned from other excellent kernels, shoutout to:\n",
        "- @bminixhofer: https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version\n",
        "- @christofhenkel: https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda\n",
        "\n",
        "Having been inspired by those kernels, there are some modifications:\n",
        "- based on my EDA kernel (https://www.kaggle.com/yosefardhito/eda-annotations-lexical-and-bag-of-words), I ignore comments with both low toxicity annotator count and toxic target value between 0.4 and 0.6 because I noticed how comments with target between that range has a questionable label even for me, let alone the model. For auxiliary attributes, I use only 'insult' because it is the only attributes significantly correlated with the target column. \n",
        "- I add a simple train/dev/test split process so I can experiment with the training data alone.\n",
        "- Paddings are added at the beginning instead of after the comment.\n",
        "- Slight modification of the Simple LSTM model on the concatenation.\n",
        "\n",
        "Why LSTM? I know from the stories of the top leaderboards that BERT and XLNet provide the best result. However, since the goal of this kernel is to learn, I prefer training a model with simpler and modifiable architecture. I want to emphasize on learning how to do it in PyTorch instead. Surely, pre-trained BERT and XLNet are next to be mastered!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "NEJn0IYEVEHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "24fd1c3d-a782-4d46-dbfe-d7b7f5a3679f"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import operator\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm; tqdm.pandas()\n",
        "\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
        "from torchtext.data import Example, Field, Dataset, LabelField, Iterator, BucketIterator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# dataframe options to display the whole comments\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfVhR6D_VEHp",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGYR6nMfVEHw",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNulqlQEXE62",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "dfff194f-7d59-4e83-d4fc-417b35edbca4"
      },
      "source": [
        "# data loading\n",
        "# use only training set\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a24a4e99-53df-474e-868d-cceb2ec692a9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a24a4e99-53df-474e-868d-cceb2ec692a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 41% 5.00M/12.1M [00:00<00:00, 16.9MB/s]\n",
            "100% 12.1M/12.1M [00:00<00:00, 30.7MB/s]\n",
            "Downloading test_public_expanded.csv.zip to /content\n",
            " 32% 5.00M/15.9M [00:00<00:00, 17.0MB/s]\n",
            "100% 15.9M/15.9M [00:00<00:00, 45.6MB/s]\n",
            "Downloading toxicity_individual_annotations.csv.zip to /content\n",
            " 76% 49.0M/64.7M [00:00<00:00, 53.0MB/s]\n",
            "100% 64.7M/64.7M [00:00<00:00, 94.8MB/s]\n",
            "Downloading identity_individual_annotations.csv.zip to /content\n",
            " 73% 9.00M/12.3M [00:00<00:00, 24.3MB/s]\n",
            "100% 12.3M/12.3M [00:00<00:00, 31.0MB/s]\n",
            "Downloading test_private_expanded.csv.zip to /content\n",
            " 32% 5.00M/15.8M [00:00<00:00, 33.9MB/s]\n",
            "100% 15.8M/15.8M [00:00<00:00, 77.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/224k [00:00<?, ?B/s]\n",
            "100% 224k/224k [00:00<00:00, 126MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 273M/276M [00:02<00:00, 120MB/s]\n",
            "100% 276M/276M [00:02<00:00, 105MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO0MO06zVEIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load only necessary fields to reduce memory footprint\n",
        "fields = ['id', 'comment_text', 'toxicity_annotator_count', 'insult', 'target']\n",
        "# load training set\n",
        "train_df = pd.read_csv(\"train.csv.zip\", usecols=fields).set_index('id')\n",
        "X_train = train_df['comment_text']\n",
        "y_train = train_df['target'] >= 0.5  # create binary target column\n",
        "train_df['is_toxic'] = y_train\n",
        "train_df['is_insult'] = train_df['insult'] >= 0.5\n",
        "# split training set to train/dev/test set with stratified strategy\n",
        "X_train, X_dev_test, y_train, y_dev_test = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=1337)\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_dev_test, y_dev_test, stratify=y_dev_test, test_size=0.5, random_state=1338)\n",
        "# load submission set\n",
        "test_df = pd.read_csv(\"test.csv.zip\").set_index('id')\n",
        "X_submission = test_df['comment_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zuXSJFHVEIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6966eb3e-b5c5-4247-bb1f-3283b9e91aa9"
      },
      "source": [
        "print(\"Length train: {:,}; dev: {:,}; test: {:,}; submission: {:,}\".format(\n",
        "    len(X_train), len(X_dev), len(X_test), len(X_submission)\n",
        "))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length train: 1,443,899; dev: 180,487; test: 180,488; submission: 97,320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6MABreJVEJv",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Load Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSgkl0MjVEJw",
        "colab_type": "text"
      },
      "source": [
        "I only try glove embeddings during this experiment. Pickled version is used to reduce time for loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhhWqCZdXakS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a46aec23-d9b6-4975-d24c-cfc1f29229cb"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-29 14:36:54--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2020-04-29 14:36:54--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2020-04-29 14:36:54--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  1.97MB/s    in 16m 54s \n",
            "\n",
            "2020-04-29 14:53:49 (2.05 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwy18-AzXp8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "287e8ea9-30b2-492f-d4d2-f27464d1cdd8"
      },
      "source": [
        "!unzip glove.840B.300d.zip\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6xn6tOVEJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4d8cb75a-042c-4c99-aa86-9dbd5572a6f6"
      },
      "source": [
        "# def load_embeddings(path):\n",
        "#     with open(path,'rb') as f:\n",
        "#         emb_arr = pickle.load(f)\n",
        "#     return emb_arr\n",
        "\n",
        "def loadGloveModel(gloveFile):\n",
        "    print (\"Loading Glove Model\")\n",
        "    f = open(gloveFile,'r', encoding='utf8')\n",
        "    model = {}\n",
        "    for line in f:\n",
        "        splitLine = line.split(' ')\n",
        "        word = splitLine[0]\n",
        "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
        "        model[word] = embedding\n",
        "    print (\"Done.\",len(model),\" words loaded!\")\n",
        "    return model\n",
        "\n",
        "EMBEDDING_PATH = \"glove.840B.300d.txt\"\n",
        "embeddings = loadGloveModel(EMBEDDING_PATH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model\n",
            "Done. 2196016  words loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C17qR50lVEKG",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suakhQxDVEKI",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xYPo17oVEKK",
        "colab_type": "text"
      },
      "source": [
        "Based on the EDA, I choose to ignore comments with target value between 0.4 and 0.6. Also, because the training set consists of an abundant number of comments with target value = 0, I decided only to sample comments that have both really low value for target and insult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx_KuFvCVEKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# take all comments with toxicity annotator count >= 10 ** 1.5\n",
        "# train_trust_df = train_df[(train_df.index.isin(X_train.index)) & (train_df['toxicity_annotator_count'] >= 10 ** 1.5)]\n",
        "# filter comments with toxicity annotator count < 10 ** 1.5 based on the target value\n",
        "# train_chosen_df = train_df[train_df.index.isin(X_train.index) & (train_df['toxicity_annotator_count'] < 10 ** 1.5)]\n",
        "train_chosen_df = train_df[train_df.index.isin(X_train.index)]\n",
        "# for safe comments, take only those with both target and insult value < 0.2 (\"very safe\")\n",
        "train_chosen_very_safe_df = train_chosen_df[(train_chosen_df['target'] < 0.2) & (train_chosen_df['insult'] < 0.2)]\n",
        "# for toxic comments, ignore those between 0.4 and 0.5\n",
        "train_chosen_toxic_df = train_chosen_df[train_chosen_df['target'] >= 0.6]\n",
        "train_chosen_others_df = train_chosen_df[(train_chosen_df['target'] >= 0.2) & (train_chosen_df['target'] < 0.4)]\n",
        "# take maximum 1mil to fit processing within 2-hour time limit for submission and the given memory size\n",
        "# n_very_safe = min(len(train_chosen_very_safe_df), \n",
        "#                   1100032 - len(train_trust_df) - len(train_chosen_others_df) - len(train_chosen_toxic_df))\n",
        "n_very_safe = min(len(train_chosen_very_safe_df), \n",
        "                  1100032 - len(train_chosen_others_df) - len(train_chosen_toxic_df))\n",
        "train_chosen_very_safe_sample_df = train_chosen_very_safe_df.sample(n=n_very_safe, random_state=1337)\n",
        "# get all valid comment indices\n",
        "train_indices = [\n",
        "    #*train_trust_df.index.values,\n",
        "    *train_chosen_very_safe_sample_df.index.values, \n",
        "    *train_chosen_others_df.index.values, \n",
        "    *train_chosen_toxic_df.index.values\n",
        "]\n",
        "X_train = X_train[X_train.index.isin(train_indices)]\n",
        "X_dev = X_dev.sample(frac=1, random_state=1337)\n",
        "X_test = X_test.sample(frac=1, random_state=1337)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9csRDNDDVEKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d884db6-902f-408f-e85b-c44a2c131c5b"
      },
      "source": [
        "print(\"Length train: {:,}; dev: {:,}; test: {:,}; submission: {:,}\".format(\n",
        "    len(X_train), len(X_dev), len(X_test), len(X_submission)\n",
        "))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length train: 1,100,032; dev: 180,487; test: 180,488; submission: 97,320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzNwIV9_VEKt",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Prepare Accepted Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwoDm2H-VEKu",
        "colab_type": "text"
      },
      "source": [
        "Because I use embeddings, there are some symbols that do not have the respective embedding vectors. In this section, I check all characters used in the training set. The character set is compared to glove embeddings symbols afterward. Instead of using slow `replace()` function, we will use `translate()`. Basically, `translate()` take a dictionary of character mapping as input. The key of the dictionary is the ordinal of the characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU0m8M6bVEKv",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "6419199ad4a14c7bac6e15b73ada4698"
          ]
        },
        "outputId": "de02f300-5bd7-4c74-8a1e-d1903914544f"
      },
      "source": [
        "def build_character_set(sentences, verbose=True):\n",
        "    characters_in_dataset = {}\n",
        "    for sentence in tqdm(sentences, disable=(not verbose)):\n",
        "        for character in sentence:\n",
        "            try:\n",
        "                characters_in_dataset[character] += 1\n",
        "            except KeyError:\n",
        "                characters_in_dataset[character] = 1\n",
        "    return characters_in_dataset\n",
        "\n",
        "train_charset = build_character_set(X_train)\n",
        "# build all accepted characters\n",
        "latin_similar = \" ’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
        "white_list = string.ascii_letters + string.digits + latin_similar\n",
        "# check all symbols that have embeddings to keep it\n",
        "glove_symbols = [symbol for symbol in embeddings if len(symbol) == 1 and symbol not in white_list]\n",
        "char_translate_map = {}\n",
        "# isolate symbols that have embeddings vector in glove\n",
        "for symbol in glove_symbols:\n",
        "    char_translate_map[ord(symbol)] = ' ' + symbol + ' '\n",
        "# remove all unknown symbols\n",
        "unknown_symbols = [c for c in train_charset if c not in white_list and c not in glove_symbols]\n",
        "for symbol in unknown_symbols:\n",
        "    char_translate_map[ord(symbol)] = ''\n",
        "# replace newline characters with normal separator (blank space)\n",
        "char_translate_map[ord('\\n')] = ' ' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6419199ad4a14c7bac6e15b73ada4698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1100032), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5q2e4_sVEMa",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XocFv6tVEMc",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "c3b1041a33cb4365837eb8eceac9a0e8"
          ]
        },
        "outputId": "dfc8325d-8e34-450d-c00b-658676e440a6"
      },
      "source": [
        "def find_in_embedding(word, embedding):\n",
        "    '''\n",
        "    find the word's vector in the embedding. \n",
        "    If there is no exact word found, try also with lower case and title case.\n",
        "    In case there is no entry found in the embedding, replace with the unknown token (<unk>)\n",
        "    '''\n",
        "    if word in embedding:\n",
        "        return word, embedding[word]\n",
        "    elif word.lower() in embedding:\n",
        "        return word.lower(), embedding[word.lower()]\n",
        "    elif word.title() in embedding:\n",
        "        return word.title(), embedding[word.title()]\n",
        "    else:\n",
        "        return '<unk>', embedding['<unk>']\n",
        "\n",
        "def preprocess(comment, translate_map, tokenizer, emb):\n",
        "    # translate symbols\n",
        "    preprocessed_comment = comment.translate(translate_map)\n",
        "    # handle quote at the beginning\n",
        "    # because we accepted quotes as characters, sometimes we encounter token such as \"'we\".\n",
        "    # with this step, we transform \"'we\" into \"' we\" and such that it can be tokenized as 2 tokens.\n",
        "    preprocessed_comment = [(\"' \" + token[1:]) if len(token) > 0 and token[0] == \"'\" else token for token in preprocessed_comment.split(' ')]\n",
        "    # tokenize with Treebank tokenizer\n",
        "    tokenized_comment = tokenizer.tokenize(' '.join(preprocessed_comment))\n",
        "    # check and find each token in embeddings\n",
        "    tokenized_comment = [find_in_embedding(token, emb)[0] for token in tokenized_comment]\n",
        "    return ' '.join(tokenized_comment)\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "X_train = X_train.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))\n",
        "X_dev = X_dev.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))\n",
        "X_test = X_test.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))\n",
        "X_submission = X_submission.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3b1041a33cb4365837eb8eceac9a0e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1100032), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgfh4FHYVENE",
        "colab_type": "code",
        "colab": {},
        "outputId": "fd109537-ea74-4081-fa27-06fa2fdf594e"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "6183788    So why is the clinton / fbi / russian colusion story nowhere on ADN ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
              "919513     Three federal judges have now issued stays of the executive order of varying lengths . Judge Ann Donnelly issued her stay in effect until a full hearing on 21 Feb 2017 and ordered the government to produce a list of all persons currently detained covered by her order which includes people on valid visas of all kinds and green - card holders . . Lawyers representing the government displayed a clear lack of information , echoing the confusion of various government agencies and officials in the past 24 hours , who had been implementing the order haphazardly . “ Things have unfolded with such speed , that we haven ’ t had time to review the legal situation yet , ” an attorney representing the government said . This prompted Judge Donnelly to observe “ I think the government hasn ’ t had a full chance to think about this . ” Sounds about par for the course with Trump . Judges Leonie Brinkema ( 7 day restraining order ) and Judge Thomas Zilly ( a stay to 03 February 2017 ) were the other federal judges .\n",
              "922616     The author seems to have completely rejected the notion of representative government .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
              "371400     You must be one of those people who do n't own a gun either where is the muzzle flash lmao . Get a gun and shoot your eye out you will lmao . You must be from the Christmas story little ralphy lol .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
              "5616964    aaa the alt left lunatic bs attempt at an argument based on fantasy yet no business realities                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
              "Name: comment_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkGWH9OQVENR",
        "colab_type": "text"
      },
      "source": [
        "# 3. Data Preparation for PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrHFS3jnVENS",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Create PyTorch Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qXPL24IVENT",
        "colab_type": "text"
      },
      "source": [
        "Before we can train a pytorch model, we have to transform the input to the form that is expected by the model. One option is to use keras text preprocessing, but I'd rather stick with pytorch family, i.e. torchtext. First, we want to transform the input into a torchtext `Dataset`. A `Dataset` is a collection of `Example`, which consists of `Field`. More info: https://torchtext.readthedocs.io/en/latest/data.html\n",
        "\n",
        "A `Field` has many useful options for NLP, among many of them are:\n",
        "- `pad_first=True` to set padding tokens added at the beginning instead of after the comment\n",
        "- `fix_length=None`, a default, to have varying length\n",
        "- `sequential=True` means the field is to be treated a sequential data. For text, set it to `True`, set otherwise for labels.\n",
        "- `use_vocab=True` whether to build vocabulary based on this field. Set to `True` for the text fields.\n",
        "\n",
        "The downside of this approach is the slowness due to the iteration to create the `Example` and `Dataset`. There should be a better way to convert the input to torchtext `Dataset`. There is also a `TabularDataset` class which can transform the whole dataset at one, but I am not aware how to include the preprocessing required without exporting the preprocessed dataset beforehand. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8-rSNcRVENY",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "d2dd7ce4377a4a0797319a295aca37b4"
          ]
        },
        "outputId": "6cb2460f-c743-4b13-b911-f0249815fa4a"
      },
      "source": [
        "def prepare_torch_dataset(comments, target, fields):\n",
        "    '''\n",
        "    transform a series `comments` to torchtext `Dataset`\n",
        "    '''\n",
        "    df = pd.DataFrame(comments).join(target[['is_toxic', 'is_insult']], how='left')\n",
        "    train_examples = [Example.fromlist(i, fields) for i in tqdm(df.values.tolist())]\n",
        "    return Dataset(train_examples, fields)\n",
        "\n",
        "# define the Fields\n",
        "ID = Field(sequential=False, use_vocab=False, dtype=torch.long)\n",
        "TEXT = Field(sequential=True, tokenize='spacy', pad_first=True)\n",
        "LABEL = LabelField(sequential=False, use_vocab=False, dtype=torch.float32)\n",
        "fields = [(\"comment_text\", TEXT), (\"target\", LABEL), (\"insult\", LABEL)]\n",
        "# prepare train/dev/test datasets as torch `Dataset`\n",
        "train_dataset = prepare_torch_dataset(X_train, train_df, fields)\n",
        "dev_dataset = prepare_torch_dataset(X_dev, train_df, fields)\n",
        "test_dataset = prepare_torch_dataset(X_test, train_df, fields)\n",
        "# prepare submission dataset as torch `Dataset`\n",
        "test_df['comment_text'] = X_submission\n",
        "subm_fields = [(\"id\", ID), (\"comment_text\", TEXT)]  # notice that there is no label field for the submission dataset\n",
        "subm_examples = [Example.fromlist(i, subm_fields) for i in tqdm(test_df.reset_index().values.tolist())]\n",
        "subm_dataset = Dataset(subm_examples, subm_fields)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2dd7ce4377a4a0797319a295aca37b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1100032), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQFdPP_wVENe",
        "colab_type": "text"
      },
      "source": [
        "Quite handily, torchtext also provides us with `build_vocab()` capability. This function helps us with the mapping between token and integer, useful for embeddings. Additionally, it contains the frequency of each word as well. `max_size` parameter determines maximum number of words registered in the vocabulary, truncated based on the frequency (top `max_size` words with the highest frequency will be used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIu3PE-2VENf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_matrix(vocab_itos, emb):\n",
        "    embedding_matrix = np.zeros((len(vocab_itos), 300))\n",
        "    for i, word in enumerate(vocab_itos):\n",
        "        embedding_matrix[i] = find_in_embedding(word, emb)[1]\n",
        "    return embedding_matrix\n",
        "\n",
        "TEXT.build_vocab(train_dataset, dev_dataset, max_size=60000)\n",
        "embedding_matrix = build_matrix(TEXT.vocab.itos, embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ibTGDNkVENl",
        "colab_type": "code",
        "colab": {},
        "outputId": "66f535bb-6c3e-464d-a31e-da0f4483314a"
      },
      "source": [
        "for i in range(20):\n",
        "    word = TEXT.vocab.itos[i]\n",
        "    print(\"id={}, token='{}', freq={}\".format(i, word, TEXT.vocab.freqs[word]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id=0, token='<unk>', freq=0\n",
            "id=1, token='<pad>', freq=0\n",
            "id=2, token='.', freq=4914165\n",
            "id=3, token='the', freq=3043813\n",
            "id=4, token=',', freq=2535539\n",
            "id=5, token='to', freq=1868569\n",
            "id=6, token='`', freq=1698286\n",
            "id=7, token='and', freq=1518146\n",
            "id=8, token='of', freq=1441722\n",
            "id=9, token='a', freq=1341631\n",
            "id=10, token='is', freq=1087585\n",
            "id=11, token='in', freq=942546\n",
            "id=12, token='that', freq=903137\n",
            "id=13, token='-', freq=806483\n",
            "id=14, token='I', freq=742814\n",
            "id=15, token='for', freq=659827\n",
            "id=16, token='it', freq=610428\n",
            "id=17, token='you', freq=606003\n",
            "id=18, token='?', freq=554502\n",
            "id=19, token='are', freq=532586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia44vjkHVENr",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Sequence Bucketing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvPXDKmJVENr",
        "colab_type": "text"
      },
      "source": [
        "In this dataset, the comment length ranges from 1 word to hundreds. If we take the maximum length of a comment and pad everything according to the maximum length, it won't be effective for training. Sequence bucketing can help reduce the time required to train. We reorder the comments based on their length, such that comments with similar length is more likely to belong on the same batch. At each batch, pad every comments to the maximum comment length on that batch. Therefore, we do not need to pad 1-word comment to the max length, and training can be much quicker.\n",
        "\n",
        "I avoid truncating any of the comment because it is misleading to decide either truncate from beginning or from end. A comment can be toxic just because 1 or 2 words it contains and those words can be at the beginning or at the end. Consequently, some outlier lengthy comments will affect the padding of much shorter ones.\n",
        "\n",
        "In torchtext, we can do sequence bucketing with `BucketIterator` class. The important parameters are `sort_key` and `sort_within_batch`. Basically, `sort_key=True` does what we expect from sequence bucketing and it is no longer necessary to also set `sort_within_batch=True` because the comments in one batch will be padded to the maximum length of that batch anyway."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iptqfQeZVENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, dev_iter, test_iter, subm_iter = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset, subm_dataset),\n",
        "    batch_sizes=(64, 64, 64, 64),\n",
        "    device=0,\n",
        "    sort_key=lambda comment: len(comment.comment_text),\n",
        "    sort_within_batch=False,\n",
        "    repeat=False  # do not repeat any inputs\n",
        ")\n",
        "\n",
        "class BatchWrapper:\n",
        "    '''\n",
        "    helper wrapper to iterate over `BucketIterator` during training and return (x, y).\n",
        "    from: https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, dl, x_var, y_vars):\n",
        "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            # return x_var attribute of the data loader (dl) as x\n",
        "            x = getattr(batch, self.x_var)\n",
        "            # return label attributes of the data loader (dl) as y\n",
        "            y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1)\n",
        "            yield (x, y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "train_dl = BatchWrapper(train_iter, \"comment_text\", [\"target\", \"insult\"])\n",
        "dev_dl = BatchWrapper(dev_iter, \"comment_text\", [\"target\", \"insult\"])\n",
        "test_dl = BatchWrapper(test_iter, \"comment_text\", [\"target\", \"insult\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135rz5GzVENy",
        "colab_type": "text"
      },
      "source": [
        "# 4. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afGVsjkQVENz",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Network Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu72F6FkVEN0",
        "colab_type": "text"
      },
      "source": [
        "The architecture of the network follows Simple LSTM architecture (https://www.kaggle.com/thousandvoices/simple-lstm) with slight modification. These are the layers:\n",
        "1. Embedding layer (1x60000) &rarr; (60000x300)\n",
        "2. Spatial dropout of embedding with dropout rate 0.3\n",
        "3. BiLSTM 1: 300 &rarr; 150 * 2 (bi-directional)\n",
        "4. BiLSTM 2: 300 &rarr; 300\n",
        "5. Concatenation of last state of BiLSTM (both direction) with average and max pooling of all hidden state: 300 + 300 + 300 = 900\n",
        "6. Fully-connected 1: 900 of concat result &rarr; 900\n",
        "7. Fully-connected 2: 900 of concat result &rarr; 900\n",
        "8. Add result of layer 5, 6, and 7: 900\n",
        "9. Dropout rate of 0.1 from layer 8\n",
        "10. Output layer for target: 768 &rarr; 1\n",
        "11. Output layer for insult: 768 &rarr; 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FasyNUMyVEN1",
        "colab_type": "code",
        "colab": {},
        "outputId": "c8183755-7f8f-466f-e6d3-822f8e4a8ae6"
      },
      "source": [
        "class SpatialDropout(nn.Dropout2d):\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "        x = x.squeeze(2)  # (N, T, K)\n",
        "        return x\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, emb_matrix, lstm_units=150):\n",
        "        super().__init__()\n",
        "        # LAYER 1: EMBEDDING, non-trainable\n",
        "        self.embedding = nn.Embedding(*emb_matrix.shape)\n",
        "        # use glove embeddings\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32), requires_grad=False)\n",
        "        # LAYER 2: SPATIAL DROPOUT\n",
        "        self.embedding_dropout = SpatialDropout(0.3)\n",
        "        # LAYER 3: LSTM, bi-directional, output = 2 * lstm_units\n",
        "        self.lstm_units = lstm_units\n",
        "        self.lstm1 = nn.LSTM(embedding_matrix.shape[1], lstm_units, bidirectional=True)\n",
        "        # LAYER 4: LSTM, bi-directional, output = 2 * lstm_units\n",
        "        self.lstm2 = nn.LSTM(2 * lstm_units, lstm_units, bidirectional=True)\n",
        "        # LAYER 5: CONCAT, no object created\n",
        "        # LAYER 6: FC1\n",
        "        self.linear1 = nn.Linear(3 * 2 * lstm_units, 3 * 2 * lstm_units)\n",
        "        # LAYER 7: FC2\n",
        "        self.linear2 = nn.Linear(3 * 2 * lstm_units, 3 * 2 * lstm_units)\n",
        "        # LAYER 8: ADDITION of layer outputs, no object created\n",
        "        # LAYER 9: DROPOUT\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # LAYER 10: OUTPUT LAYER for target\n",
        "        self.target_out = nn.Linear(3 * 2 * lstm_units, 1)\n",
        "        # LAYER 11: OUTPUT LAYER for insult\n",
        "        self.aux_out = nn.Linear(3 * 2 * lstm_units, 1)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        # get embedding vector of every word\n",
        "        h_emb = self.embedding_dropout(self.embedding(seq))\n",
        "        # walk through the first bi-directional LSTM\n",
        "        h_lstm1, _ = self.lstm1(h_emb)\n",
        "        # use output of previous LSTM as input for the second bi-directional LSTM\n",
        "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
        "        # get the latest state of LSTM 2\n",
        "        # because it is bi-directional, get the first half output of the last state (LSTM direction forward)\n",
        "        # and the second half output of the first state (LSTM direction backward)\n",
        "        h_lstm_last = h_lstm2[-1, :, :self.lstm_units]\n",
        "        h_lstm_first = h_lstm2[0, :, self.lstm_units:]\n",
        "        # concat both half\n",
        "        h_bi_lstm = torch.cat((h_lstm_first, h_lstm_last), -1)\n",
        "        # get average and max pool of all hidden state from LSTM 2\n",
        "        avg_pool = torch.mean(h_lstm2, 0)\n",
        "        max_pool, _ = torch.max(h_lstm2, 0)\n",
        "        # concat last state with pooling\n",
        "        h_conc = torch.cat((h_bi_lstm, max_pool, avg_pool), 1)\n",
        "        # fully-connected layer with last state and the pooling\n",
        "        h_lin1 = F.relu(self.linear1(h_conc))\n",
        "        h_lin2 = F.relu(self.linear2(h_conc))\n",
        "        # add concat with output of fully-connected layer\n",
        "        h_conc_add = h_conc + h_lin1 + h_lin2\n",
        "        # another dropout to force model check last state, average pool, and max pool values\n",
        "        h_conc_do = self.dropout(h_conc_add)\n",
        "        # get linear prediction of target and insult\n",
        "        # we do not use sigmoid here because pyTorch provides a better loss calculation\n",
        "        # by combining sigmoid with cross-entropy\n",
        "        target_out = self.target_out(h_conc_do)\n",
        "        aux_out = self.aux_out(h_conc_do)\n",
        "        out = torch.cat([target_out, aux_out], 1)\n",
        "        return out\n",
        "    \n",
        "model = LSTM(embedding_matrix).to(device)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(60002, 300)\n",
              "  (embedding_dropout): SpatialDropout(p=0.3)\n",
              "  (lstm1): LSTM(300, 150, bidirectional=True)\n",
              "  (lstm2): LSTM(300, 150, bidirectional=True)\n",
              "  (linear1): Linear(in_features=900, out_features=900, bias=True)\n",
              "  (linear2): Linear(in_features=900, out_features=900, bias=True)\n",
              "  (dropout): Dropout(p=0.1)\n",
              "  (target_out): Linear(in_features=900, out_features=1, bias=True)\n",
              "  (aux_out): Linear(in_features=900, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSs7r2esVEOC",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTSwWDCYVEOD",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "7494bd163a444767b725a03ee76bf060"
          ]
        },
        "outputId": "f9332408-98ae-4ec1-f4c7-20c7f01dcaeb"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def evaluate(model, dl, loss_func):\n",
        "    '''\n",
        "    evaluation steps for dev and test set\n",
        "    '''\n",
        "    running_loss = 0.0\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    all_preds = {'y_pred' : [], 'y_true' : []}\n",
        "    for x, y in dl: # iterate over mini-batches\n",
        "        x, y = x.to(device), y.to(device)  # transfer x and y to GPU when it is available\n",
        "        preds = model(x)  # get the prediction from the model\n",
        "        loss = loss_func(preds, y)  # calculate loss values\n",
        "        # add all predictions and y_true to a dictionary for calculating the evaluation metrics\n",
        "        all_preds['y_pred'] = [*all_preds['y_pred'], *sigmoid(preds.detach().cpu().numpy()[:,0].ravel())]\n",
        "        all_preds['y_true'] = [*all_preds['y_true'], *y.detach().cpu().numpy()[:,0].ravel()]\n",
        "        # add current mini-batch loss to the total loss\n",
        "        running_loss += loss.item() / len(dl)\n",
        "    # create a binary prediction to calculate accuracy\n",
        "    all_preds['y_pred_bin'] = [1 if pred >= 0.5 else 0 for pred in all_preds['y_pred']]\n",
        "    print('\\tEval Loss: {:.4f}, Eval AUC: {:.4f}, Eval Accuracy: {:.4f}'.format(\n",
        "        loss,\n",
        "        roc_auc_score(all_preds['y_true'], all_preds['y_pred']),\n",
        "        accuracy_score(all_preds['y_true'], all_preds['y_pred_bin'])\n",
        "    ))\n",
        "    # confusion analysis, especially helpful for imbalanced dataset\n",
        "    tn, fp, fn, tp = confusion_matrix(all_preds['y_true'], all_preds['y_pred_bin']).ravel()\n",
        "    print('\\tEval CM: tn={:,}({:.2f}%), tp={:,}({:.2f}%), fn={:,}({:.2f}%), fp={:,}({:.2f}%)'.format(\n",
        "        tn, tn * 100 / len(all_preds['y_pred_bin']),\n",
        "        tp, tp * 100 / len(all_preds['y_pred_bin']),\n",
        "        fn, fn * 100 / len(all_preds['y_pred_bin']),\n",
        "        fp, fp * 100 / len(all_preds['y_pred_bin'])\n",
        "    ))\n",
        "    \n",
        "    \n",
        "def train(model, train, dev, n_epochs, loss_func, lr):\n",
        "    '''\n",
        "    model training steps for the training set. Output dev metrics at each epoch.\n",
        "    PyTorch requires implementation of training procedure.\n",
        "    '''\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)  # optimizer\n",
        "    # degrade learning rate at every epoch with lr scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.3)\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print('Epoch: {}'.format(epoch))\n",
        "        model.train()  # during training, dropout is activated\n",
        "        running_loss = 0.0\n",
        "        for x, y in tqdm(train):\n",
        "            opt.zero_grad()  # necessary at the beginning of each mini-batch for pytorch\n",
        "            x, y = x.to(device), y.to(device)  # transfer to GPU when available\n",
        "            preds = model(x)  # get prediction\n",
        "            loss = loss_func(preds, y)  # calculate loss\n",
        "            loss.backward()  # backpropagation\n",
        "            opt.step()  # calculate optimizer with adam\n",
        "            running_loss += loss.item() / len(train_dl)  # add current mini-batch loss to total loss\n",
        "        print('\\tTraining Loss: {:.4f}'.format(running_loss))\n",
        "        evaluate(model, dev, loss_func)  # check dev scores\n",
        "        scheduler.step()  # reduce learning rate\n",
        "\n",
        "# Binary Cross-Entropy With Logits Loss: optimized loss function for binary classification\n",
        "# when using this loss function, do not use sigmoid activation function in the output layer of the model.\n",
        "bce_loss = nn.BCEWithLogitsLoss(\n",
        "    pos_weight=torch.FloatTensor([1]).cuda(),   # I tried to penalize mis-classified positive instance more\n",
        "                                                # but it did not give better LB score\n",
        "    reduction='mean'  # because we are predicting 2 outputs, take the mean as loss value\n",
        ")\n",
        "train(model, train_dl, dev_dl, 2, bce_loss, lr=8e-4)  # run training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7494bd163a444767b725a03ee76bf060",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=17188), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4CDUYtGVETP",
        "colab_type": "code",
        "colab": {},
        "outputId": "f3e20c23-0d26-437e-94f8-409804ad658d"
      },
      "source": [
        "evaluate(model, test_dl, bce_loss)  # evaluate on test set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEval Loss: 0.0266, Eval AUC: 0.9590, Eval Accuracy: 0.9503\n",
            "\tEval CM: tn=162,695(90.14%), tp=8,830(4.89%), fn=5,604(3.10%), fp=3,359(1.86%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd27gChtVEUE",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "5ff568d181fe44cb8b6ca041ad32748f"
          ]
        },
        "outputId": "389e3af5-f745-44a7-98c2-734b91f862d1"
      },
      "source": [
        "# fine-tuning embedding layer\n",
        "model.embedding.weight.requires_grad = True\n",
        "train(model, train_dl, dev_dl, 1, bce_loss, lr=8e-5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ff568d181fe44cb8b6ca041ad32748f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=17188), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCE71yVuVEUL",
        "colab_type": "code",
        "colab": {},
        "outputId": "c3da62f2-5c8c-4865-f14d-98ce0a01d3e5"
      },
      "source": [
        "evaluate(model, test_dl, bce_loss)  # evaluate on test set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEval Loss: 0.0212, Eval AUC: 0.9614, Eval Accuracy: 0.9523\n",
            "\tEval CM: tn=163,079(90.35%), tp=8,795(4.87%), fn=5,639(3.12%), fp=2,975(1.65%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21PTcXjlVEVb",
        "colab_type": "text"
      },
      "source": [
        "The evaluation metrics used in this loop is basic AUC ROC, whereas the competition impose sub-groups AUC score in the calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St0wNZILVEVc",
        "colab_type": "text"
      },
      "source": [
        "# 5. Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B6IFWfOVEVd",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "ee509f3ce2024307bb9085d1430560f0"
          ]
        },
        "outputId": "afb9a2f9-4bda-45b4-97be-c049c9f62086"
      },
      "source": [
        "model.eval()  # turn on evaluation mode to ignore dropouts\n",
        "subm_id = []\n",
        "subm_preds = []\n",
        "for example in tqdm(subm_iter):\n",
        "    x_id = example.id\n",
        "    x = example.comment_text.to(device)\n",
        "    preds = model(x)\n",
        "    subm_id = [*subm_id, *x_id.detach().cpu().numpy().ravel()]\n",
        "    subm_preds = [*subm_preds, *sigmoid(preds.detach().cpu().numpy()[:,0].ravel())]\n",
        "submission = pd.DataFrame.from_dict({\n",
        "    'id': subm_id,\n",
        "    'prediction': subm_preds\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee509f3ce2024307bb9085d1430560f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1521), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}